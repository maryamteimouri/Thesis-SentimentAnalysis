{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up"
      ],
      "metadata": {
        "id": "rDynqpotXJOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUApTUI61bvk",
        "outputId": "93247cd7-6a9c-4b6d-df05-ca9e3fb867a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.35.12-py3-none-any.whl (328 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/328.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/328.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.4/328.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.0)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.35.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key='##')"
      ],
      "metadata": {
        "id": "m_7koYK1ahOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat - SRT Corrections - CS2 Experiment\n",
        "### GPT4"
      ],
      "metadata": {
        "id": "PCApDWYwq39t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyoeHGi85qPW"
      },
      "outputs": [],
      "source": [
        "msg =  \"\"\"I have transcribed text featuring a conversation between two kids\n",
        "            speaking Finnish and playing guessing games. The transcription\n",
        "            captures only one person's dialogue and includes ordinal numbers and timestamps.\n",
        "            While the transcription is mostly accurate, there are sections with\n",
        "            unclear or incorrect wording (gibberish). Could you help me identify\n",
        "            and correct these errors, ensuring that each corrected segment is\n",
        "            clearly associated with its ordinal number and original timestamp?\n",
        "            Additionally, it’s important to maintain the text's overall format,\n",
        "            with each ordinal number on a separate line, followed by its timestamp\n",
        "            and dialogue on subsequent lines. Not all sentences may end with a period.\n",
        "            please apply corrections and print the whole text without any extra output.\n",
        "            Here is the text:\n",
        "            \\n\\n\"\"\"\n",
        "\n",
        "def process_files(file_path):\n",
        "    out_put = \"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        file_content = file.read()\n",
        "        stream = client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[{\"role\": \"user\", \"content\":f\"{msg}{file_content}\"}],\n",
        "        stream=True,\n",
        "      )\n",
        "    for chunk in stream:\n",
        "        if chunk.choices[0].delta.content is not None:\n",
        "            out_put += chunk.choices[0].delta.content\n",
        "    return out_put"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_paths = ['1.srt', 'A1_Hololens.srt', 'B1_camera.srt',\n",
        "                   'C2_HoloLens.srt', '2.srt', 'A2_Hololens.srt',\n",
        "                   'B2_camera.srt', 'D1_camera.srt', '3.srt',\n",
        "                   'A3_Hololens.srt', 'C1_Hololens.srt', 'D2_phone.srt']\n",
        "\n",
        "for path in file_paths:\n",
        "    out = process_files(path)\n",
        "    with open(f\"corrected_{path}\", 'w', encoding='utf-8') as file:\n",
        "        file.write(out)"
      ],
      "metadata": {
        "id": "OBHItD3jIoXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT3"
      ],
      "metadata": {
        "id": "byUSn-TeX52S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "msg =  \"\"\"I have transcribed text featuring a conversation between two kids\n",
        "            speaking Finnish and playing guessing games. The transcription\n",
        "            captures only one person's dialogue and includes ordinal numbers and timestamps.\n",
        "            While the transcription is mostly accurate, there are sections with\n",
        "            unclear or incorrect wording (gibberish). Could you help me identify\n",
        "            and correct these errors, ensuring that each corrected segment is\n",
        "            clearly associated with its ordinal number and original timestamp?\n",
        "            Additionally, it’s important to maintain the text's overall format,\n",
        "            with each ordinal number on a separate line, followed by its timestamp\n",
        "            and dialogue on subsequent lines. Not all sentences may end with a period.\n",
        "            please apply corrections and print the whole text without any extra output.\n",
        "            Here is the text:\n",
        "            \\n\\n\"\"\"\n",
        "\n",
        "def process_files(file_path):\n",
        "    out_put = \"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        file_content = file.read()\n",
        "        stream = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-1106\",\n",
        "        messages=[{\"role\": \"user\", \"content\":f\"{msg}{file_content}\"}],\n",
        "        stream=True,\n",
        "      )\n",
        "    for chunk in stream:\n",
        "        if chunk.choices[0].delta.content is not None:\n",
        "            out_put += chunk.choices[0].delta.content\n",
        "    return out_put"
      ],
      "metadata": {
        "id": "olCUfmpiX_dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_paths = ['1.srt', 'A1_Hololens.srt', 'B1_camera.srt',\n",
        "              'C2_HoloLens.srt', '2.srt', 'A2_Hololens.srt',\n",
        "              'B2_camera.srt', 'D1_camera.srt', '3.srt',\n",
        "              'A3_Hololens.srt', 'C1_Hololens.srt', 'D2_phone.srt']\n",
        "\n",
        "for path in file_paths:\n",
        "    out = process_files(path)\n",
        "    with open(f\"corrected_{path}\", 'w', encoding='utf-8') as file:\n",
        "        file.write(out)"
      ],
      "metadata": {
        "id": "-p2bzVYoYCvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interview CS2 Corrections\n",
        "### GPT4"
      ],
      "metadata": {
        "id": "lLKPFjJXMwEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_prompt = \"\"\"\n",
        "Hi, I have a text which is the auto-transcription of an interview in Finnish.\n",
        "The interview happens between an adult and a few kids.\n",
        "They are talking about an experiment they did last week,\n",
        "which included playing games on zoom or on a VR platform.\n",
        "The text looks fine, but it needs some editing.\n",
        "Please correct those parts in the text for me and print the entire text with the corrections made.\n",
        "Ensure that any corrections blend seamlessly with the original content, preserving the overall structure and timestamps.\n",
        "Do not introduce any extra outputs, this is a must.\n",
        "Here is the text:\n",
        "\\n\\n\"\"\"\n",
        "\n",
        "\n",
        "def process_file_in_chunks(file_path, chunk_size):\n",
        "    \"\"\"Process the file in chunks with a specified size.\"\"\"\n",
        "    output = \"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            file_content = file.readlines()\n",
        "\n",
        "        # Process in chunks\n",
        "        for i in range(0, len(file_content), chunk_size):\n",
        "            chunk = \"\".join(file_content[i:i+chunk_size])\n",
        "            processed_chunk = process_chunk(chunk)  # Define process_chunk to use the API\n",
        "            output += processed_chunk\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    return output\n",
        "\n",
        "def process_chunk(chunk_text):\n",
        "    \"\"\"Process a single chunk of text. Implement API calling and response handling here.\"\"\"\n",
        "    # This function is a placeholder for where you would make the API call.\n",
        "    # You'd need to implement making the API request with the chunk_text here.\n",
        "    processed_text = \"\"\n",
        "    stream = client.chat.completions.create(\n",
        "          model=\"gpt-4\",\n",
        "          messages=[{\"role\": \"user\", \"content\":f\"{gpt_prompt}{chunk_text}\"}],\n",
        "          stream=True,\n",
        "    )\n",
        "    for chunk in stream:\n",
        "          if chunk.choices[0].delta.content is not None:\n",
        "              processed_text += chunk.choices[0].delta.content\n",
        "    return processed_text\n"
      ],
      "metadata": {
        "id": "u7KScMnhm1vF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_paths = ['2_interview.srt', '3_interview.srt', #'1_interview.srt'\n",
        "              '4_interview.srt']\n",
        "chunk_size = 100\n",
        "for path in file_paths:\n",
        "    output = process_file_in_chunks(path, chunk_size)  # Adjust chunk_size as needed\n",
        "    with open(f\"corrected_{path}\", 'w', encoding='utf-8') as file:\n",
        "        file.write(output)"
      ],
      "metadata": {
        "id": "azHz95AWwYrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT3"
      ],
      "metadata": {
        "id": "EHOEGFOF8nXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_prompt = \"\"\"\n",
        "Hi, I have a text which is the auto-transcription of an interview in Finnish.\n",
        "The interview happens between an adult and a few kids.\n",
        "They are talking about an experiment they did last week,\n",
        "which included playing games on zoom or on a VR platform.\n",
        "The text looks fine, but it needs some editing.\n",
        "Please correct those parts in the text for me and print the entire text with the corrections made.\n",
        "Ensure that any corrections blend seamlessly with the original content, preserving the overall structure and timestamps.\n",
        "Do not introduce any extra outputs, this is a must.\n",
        "Here is the text:\n",
        "\\n\\n\"\"\"\n",
        "\n",
        "\n",
        "def process_file_in_chunks(file_path, chunk_size):\n",
        "    \"\"\"Process the file in chunks with a specified size.\"\"\"\n",
        "    output = \"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            file_content = file.readlines()\n",
        "\n",
        "        # Process in chunks\n",
        "        for i in range(0, len(file_content), chunk_size):\n",
        "            chunk = \"\".join(file_content[i:i+chunk_size])\n",
        "            processed_chunk = process_chunk(chunk)  # Define process_chunk to use the API\n",
        "            output += processed_chunk\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    return output\n",
        "\n",
        "def process_chunk(chunk_text):\n",
        "    \"\"\"Process a single chunk of text. Implement API calling and response handling here.\"\"\"\n",
        "    # This function is a placeholder for where you would make the API call.\n",
        "    # You'd need to implement making the API request with the chunk_text here.\n",
        "    processed_text = \"\"\n",
        "    stream = client.chat.completions.create(\n",
        "          model=\"gpt-3.5-turbo-1106\",\n",
        "          messages=[{\"role\": \"user\", \"content\":f\"{gpt_prompt}{chunk_text}\"}],\n",
        "          stream=True,\n",
        "    )\n",
        "    for chunk in stream:\n",
        "          if chunk.choices[0].delta.content is not None:\n",
        "              processed_text += chunk.choices[0].delta.content\n",
        "    return processed_text\n"
      ],
      "metadata": {
        "id": "wOE1B2vT8p94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_paths = ['1_interview.srt', '2_interview.srt', '3_interview.srt',\n",
        "              '4_interview.srt']\n",
        "chunk_size = 100\n",
        "for path in file_paths:\n",
        "    output = process_file_in_chunks(path, chunk_size)  # Adjust chunk_size as needed\n",
        "    with open(f\"corrected_{path}\", 'w', encoding='utf-8') as file:\n",
        "        file.write(output)"
      ],
      "metadata": {
        "id": "0NYs6e0m8qVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interviews CS1 Corrections\n",
        "### **GPT4**"
      ],
      "metadata": {
        "id": "uN3Dz0EDKJjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_prompt = \"\"\"\n",
        "Hi, I have a text which is the auto-transcription of an interview in Finnish.\n",
        "The interview happens between two adults.\n",
        "They are talking about an experiment they did last week,\n",
        "which was teaching kids on zoom or on a VR platform.\n",
        "The text looks fine, but it needs some editing.\n",
        "Please correct those parts in the text for me and print the entire text with the corrections made.\n",
        "Ensure that any corrections blend seamlessly with the original content, preserving the overall structure and timestamps.\n",
        "Do not introduce any extra outputs, this is a must.\n",
        "Here is the text:\n",
        "\\n\\n\"\"\"\n",
        "\n",
        "\n",
        "def process_file_in_chunks(file_path, chunk_size):\n",
        "    \"\"\"Process the file in chunks with a specified size.\"\"\"\n",
        "    output = \"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            file_content = file.readlines()\n",
        "\n",
        "        # Process in chunks\n",
        "        for i in range(0, len(file_content), chunk_size):\n",
        "            chunk = \"\".join(file_content[i:i+chunk_size])\n",
        "            processed_chunk = process_chunk(chunk)  # Define process_chunk to use the API\n",
        "            output += processed_chunk\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    return output\n",
        "\n",
        "def process_chunk(chunk_text):\n",
        "    \"\"\"Process a single chunk of text. Implement API calling and response handling here.\"\"\"\n",
        "    # This function is a placeholder for where you would make the API call.\n",
        "    # You'd need to implement making the API request with the chunk_text here.\n",
        "    processed_text = \"\"\n",
        "    stream = client.chat.completions.create(\n",
        "          model=\"gpt-4\",\n",
        "          messages=[{\"role\": \"user\", \"content\":f\"{gpt_prompt}{chunk_text}\"}],\n",
        "          stream=True,\n",
        "    )\n",
        "    for chunk in stream:\n",
        "          if chunk.choices[0].delta.content is not None:\n",
        "              processed_text += chunk.choices[0].delta.content\n",
        "    return processed_text\n"
      ],
      "metadata": {
        "id": "EZJWQmUoKQBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_paths = ['CS1_interview_henna.srt', 'CS1_interview_konsta.srt',\n",
        "              'CS1_interview_sanna.srt', 'CS1_interview_students.srt']\n",
        "chunk_size = 100\n",
        "for path in file_paths:\n",
        "    output = process_file_in_chunks(path, chunk_size)  # Adjust chunk_size as needed\n",
        "    with open(f\"corrected_{path}\", 'w', encoding='utf-8') as file:\n",
        "        file.write(output)"
      ],
      "metadata": {
        "id": "X9XBRsVPKWdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GPT3**"
      ],
      "metadata": {
        "id": "0lRtHeAoTo6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_prompt = \"\"\"\n",
        "Hi, I have a text which is the auto-transcription of an interview in Finnish.\n",
        "The interview happens between two adults.\n",
        "They are talking about an experiment they did last week,\n",
        "which was teaching kids on zoom or on a VR platform.\n",
        "The text looks fine, but it needs some editing.\n",
        "Please correct those parts in the text for me and print the entire text with the corrections made.\n",
        "Ensure that any corrections blend seamlessly with the original content, preserving the overall structure and timestamps.\n",
        "Do not introduce any extra outputs, this is a must.\n",
        "Here is the text:\n",
        "\\n\\n\"\"\"\n",
        "\n",
        "\n",
        "def process_file_in_chunks(file_path, chunk_size):\n",
        "    \"\"\"Process the file in chunks with a specified size.\"\"\"\n",
        "    output = \"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            file_content = file.readlines()\n",
        "\n",
        "        # Process in chunks\n",
        "        for i in range(0, len(file_content), chunk_size):\n",
        "            chunk = \"\".join(file_content[i:i+chunk_size])\n",
        "            processed_chunk = process_chunk(chunk)  # Define process_chunk to use the API\n",
        "            output += processed_chunk\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    return output\n",
        "\n",
        "def process_chunk(chunk_text):\n",
        "    \"\"\"Process a single chunk of text. Implement API calling and response handling here.\"\"\"\n",
        "    # This function is a placeholder for where you would make the API call.\n",
        "    # You'd need to implement making the API request with the chunk_text here.\n",
        "    processed_text = \"\"\n",
        "    stream = client.chat.completions.create(\n",
        "          model=\"gpt-3.5-turbo-1106\",\n",
        "          messages=[{\"role\": \"user\", \"content\":f\"{gpt_prompt}{chunk_text}\"}],\n",
        "          stream=True,\n",
        "    )\n",
        "    for chunk in stream:\n",
        "          if chunk.choices[0].delta.content is not None:\n",
        "              processed_text += chunk.choices[0].delta.content\n",
        "    return processed_text\n"
      ],
      "metadata": {
        "id": "gtJ8CO6OTsVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_paths = ['CS1_interview_henna.srt', 'CS1_interview_konsta.srt',\n",
        "              'CS1_interview_sanna.srt', 'CS1_interview_students.srt']\n",
        "chunk_size = 100\n",
        "for path in file_paths:\n",
        "    output = process_file_in_chunks(path, chunk_size)  # Adjust chunk_size as needed\n",
        "    with open(f\"corrected_{path}\", 'w', encoding='utf-8') as file:\n",
        "        file.write(output)"
      ],
      "metadata": {
        "id": "4KxD_xZtTsqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Translations"
      ],
      "metadata": {
        "id": "5OgGF8pQRA60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CS1_prompt = \"\"\"\n",
        "Hi, I have a text which is the auto-transcription of an interview in Finnish.\n",
        "The interview happens between two adults.\n",
        "They are talking about an experiment they did last week,\n",
        "which was teaching kids on zoom or on a VR platform. Can you translate\n",
        "the text to English? please preserve the overall structure and timestamps.\n",
        "Do not introduce any extra outputs.\n",
        "Here is the text:\n",
        "\\n\\n\"\"\"\n",
        "\n",
        "CS2_exp_prompt_2kids = \"\"\"\n",
        "Hi, I have a text which is the auto-transcription of an interview in Finnish.\n",
        "a conversation between two kids speaking in Finnish and playing guessing games\n",
        "Over zoom or VR platform. the text was previously\n",
        "edited by gpt-4 model because it had some gibberish parts. Can you translate\n",
        "it to English? please preserve the overall structure and timestamps.\n",
        "Do not introduce any extra outputs.\n",
        "Here is the text:\n",
        "\\n\\n\"\"\"\n",
        "\n",
        "CS2_exp_prompt_1kid = \"\"\"\n",
        "Hi, I have a text which is the auto-transcription of an interview in Finnish.\n",
        "a conversation between two kids speaking in Finnish and playing guessing games\n",
        "Over zoom or VR platform. The transcription captures only one person's dialogue\n",
        "the text was previously\n",
        "edited by gpt-4 model because it had some gibberish parts. Can you translate\n",
        "it to English? please preserve the overall structure and timestamps.\n",
        "Do not introduce any extra outputs.\n",
        "Here is the text:\n",
        "\\n\\n\"\"\"\n",
        "\n",
        "CS2_int_prompt = \"\"\"\n",
        "Hi, I have a text which is the auto-transcription of an interview in Finnish.\n",
        "The interview happens between an adults and a few kids.\n",
        "They are talking about an experiment they did last week,\n",
        "which included playing games on zoom or on a VR platform. the text was previously\n",
        "edited by gpt-4 model because it had some gibberish parts. Can you translate\n",
        "it to English? please preserve the overall structure and timestamps.\n",
        "Do not introduce any extra outputs.\n",
        "Here is the text:\n",
        "\\n\\n\"\"\"\n",
        "\n",
        "def process_file_in_chunks(file_path, chunk_size):\n",
        "    \"\"\"Process the file in chunks with a specified size.\"\"\"\n",
        "    output = \"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            file_content = file.readlines()\n",
        "\n",
        "        # Process in chunks\n",
        "        for i in range(0, len(file_content), chunk_size):\n",
        "            chunk = \"\".join(file_content[i:i+chunk_size])\n",
        "            processed_chunk = process_chunk(chunk)  # Define process_chunk to use the API\n",
        "            output += processed_chunk\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    return output\n",
        "\n",
        "def process_chunk(chunk_text):\n",
        "    \"\"\"Process a single chunk of text. Implement API calling and response handling here.\"\"\"\n",
        "    # This function is a placeholder for where you would make the API call.\n",
        "    # You'd need to implement making the API request with the chunk_text here.\n",
        "    processed_text = \"\"\n",
        "    stream = client.chat.completions.create(\n",
        "          model=\"gpt-4\",\n",
        "          messages=[{\"role\": \"user\", \"content\":f\"{CS2_exp_prompt_2kids}{chunk_text}\"}],\n",
        "          stream=True,\n",
        "    )\n",
        "    for chunk in stream:\n",
        "          if chunk.choices[0].delta.content is not None:\n",
        "              processed_text += chunk.choices[0].delta.content\n",
        "    return processed_text"
      ],
      "metadata": {
        "id": "ygzsaSHXRGkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CS1_files = ['corrected_CS1_interview_henna.srt', 'corrected_CS1_interview_konsta.srt',\n",
        "             'corrected_CS1_interview_sanna.srt', 'corrected_CS1_interview_students.srt']\n",
        "\n",
        "CS2_2kid = ['corrected_1.srt', 'corrected_2.srt', 'corrected_3.srt']\n",
        "\n",
        "CS2_1kid = [\"corrected_A1_Hololens.srt\", 'corrected_B2_camera.srt',\n",
        "            'corrected_D2_phone.srt', 'corrected_A2_Hololens.srt',\n",
        "            'corrected_C1_Hololens.srt', 'corrected_A3_Hololens.srt',\n",
        "            'corrected_C2_HoloLens.srt', 'corrected_B1_camera.srt',\n",
        "            'corrected_D1_camera.srt']\n",
        "\n",
        "CS2_interviews = ['corrected_1_interview.srt', 'corrected_2_interview.srt',\n",
        "                  'corrected_3_interview.srt', 'corrected_4_interview.srt']\n",
        "\n",
        "chunk_size = 100\n",
        "for path in CS2_2kid:\n",
        "    output = process_file_in_chunks(path, chunk_size)  # Adjust chunk_size as needed\n",
        "    with open(f\"GPT_translated_{path}\", 'w', encoding='utf-8') as file:\n",
        "        file.write(output)"
      ],
      "metadata": {
        "id": "npM-qsH1RM_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speaker Marking"
      ],
      "metadata": {
        "id": "Q4BSvpO2IH1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CS1_prompt = \"\"\"\n",
        "Hi I have a text which is transcripted from audios, originally it was in Finnish\n",
        "but I translated it to English. The context\n",
        "is two adults, it is an interview about some kids in an experiment they had\n",
        "before. The kids were in classrooms over zoom or a virtual reality platform.\n",
        "can you mark who is speaking, speaker1 speaker2? please delete the timestamps\n",
        "too. Do not introduce any extra outputs.\n",
        "\\n\\n\"\"\"\n",
        "\n",
        "CS1_prompt_children = \"\"\"\n",
        "Hi I have a text which is transcripted from audios, originally it was in Finnish\n",
        "but I translated it to English. The context\n",
        "is an adult and some kids, it is an interview about some kids in an experiment they had\n",
        "before. The kids were in classrooms over zoom or a virtual reality platform.\n",
        "can you mark who is speaking? please delete the timestamps\n",
        "too. Do not introduce any extra outputs.\n",
        "\\n\\n\"\"\"\n",
        "\n",
        "CS2_exp_prompt_2kids = \"\"\"\n",
        "Hi I have a text which is transcripted from audios, in Finnish. The context is\n",
        "two kids playing over zoom or a virtual reality platform. can you delete\n",
        "the timestamps and mark who is speaking, speaker1 speaker2 or instructor.\n",
        "the instructor is not present all the time, he just gives some instructions in\n",
        "the beginning or announce something at the end or in the middle.\n",
        "Do not introduce any extra outputs.\n",
        "Here is the text:\n",
        "\\n\\n\"\"\"\n",
        "\n",
        "CS2_exp_prompt_1kid = \"\"\"\n",
        "Hi, I have a text which is the auto-transcription of an interview in Finnish.\n",
        "a conversation between two kids speaking in Finnish and playing guessing games\n",
        "Over zoom or VR platform. The transcription captures only one person's dialogue\n",
        "the text was previously\n",
        "edited by gpt-4 model because it had some gibberish parts. Can you translate\n",
        "it to English? please preserve the overall structure and timestamps.\n",
        "Do not introduce any extra outputs.\n",
        "Here is the text:\n",
        "\\n\\n\"\"\"\n",
        "\n",
        "CS2_int_prompt = \"\"\"\n",
        "Hi, I have a text which is the auto-transcription of an interview which was\n",
        "originally Finnish but I translated it to English.\n",
        "The interview happens between an interviewer who is an adult and a few students\n",
        "(3 to 5 kids) in a middle school.\n",
        "They are talking about an experiment they did last week,\n",
        "which included playing games on zoom or on a VR platform. Can you delete the\n",
        "timestamps and mark who is talking?\n",
        "Do not introduce any extra outputs.\n",
        "Here is the text:\n",
        "\\n\\n\"\"\"\n",
        "\n",
        "def process_file_in_chunks(file_path, chunk_size):\n",
        "    \"\"\"Process the file in chunks with a specified size.\"\"\"\n",
        "    output = \"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            file_content = file.readlines()\n",
        "\n",
        "        # Process in chunks\n",
        "        for i in range(0, len(file_content), chunk_size):\n",
        "            chunk = \"\".join(file_content[i:i+chunk_size])\n",
        "            processed_chunk = process_chunk(chunk)  # Define process_chunk to use the API\n",
        "            output += processed_chunk\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    return output\n",
        "\n",
        "def process_chunk(chunk_text):\n",
        "    \"\"\"Process a single chunk of text. Implement API calling and response handling here.\"\"\"\n",
        "    # This function is a placeholder for where you would make the API call.\n",
        "    # You'd need to implement making the API request with the chunk_text here.\n",
        "    processed_text = \"\"\n",
        "    stream = client.chat.completions.create(\n",
        "          model=\"gpt-4\",\n",
        "          messages=[{\"role\": \"user\", \"content\":f\"{CS2_int_prompt}{chunk_text}\"}],\n",
        "          stream=True,\n",
        "    )\n",
        "    for chunk in stream:\n",
        "          if chunk.choices[0].delta.content is not None:\n",
        "              processed_text += chunk.choices[0].delta.content\n",
        "    return processed_text"
      ],
      "metadata": {
        "id": "QRlCAhohIKwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CS1_files = [ 'CS1_interview_konsta.srt', 'CS1_interview_henna.srt',\n",
        "              'CS1_interview_sanna.srt', 'CS1_interview_students.srt']\n",
        "\n",
        "CS2_2kid = ['corrected_1.srt', 'corrected_2.srt', 'corrected_3.srt']\n",
        "\n",
        "CS2_1kid = [\"corrected_A1_Hololens.srt\", 'corrected_B2_camera.srt',\n",
        "            'corrected_D2_phone.srt', 'corrected_A2_Hololens.srt',\n",
        "            'corrected_C1_Hololens.srt', 'corrected_A3_Hololens.srt',\n",
        "            'corrected_C2_HoloLens.srt', 'corrected_B1_camera.srt',\n",
        "            'corrected_D1_camera.srt']\n",
        "\n",
        "CS2_interviews = ['1_interview.srt', '2_interview.srt', '3_interview.srt',\n",
        "                  '4_interview.srt']\n",
        "\n",
        "\n",
        "chunk_size = 100\n",
        "for path in CS2_interviews:\n",
        "    output = process_file_in_chunks(path, chunk_size)  # Adjust chunk_size as needed\n",
        "    with open(f\"Maked_{path}\", 'w', encoding='utf-8') as file:\n",
        "        file.write(output)"
      ],
      "metadata": {
        "id": "ohO3XUQBIRO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## txt to docx format"
      ],
      "metadata": {
        "id": "et7ZwDQLLYGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx\n",
        "from docx import Document\n",
        "import re\n",
        "import os"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpoBYtfjLW7W",
        "outputId": "86a1e2a6-bac0-48c4-ca55-cf9d1f1b7c47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m225.3/244.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.11.0)\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/in'\n",
        "direct = os.listdir(path)\n",
        "\n",
        "for i in direct:\n",
        "    if i == '.ipynb_checkpoints':\n",
        "        continue\n",
        "\n",
        "    document = Document()\n",
        "    document.add_heading(i, 0)\n",
        "    myfile = open('/content/in/' + i).read()\n",
        "    #myfile = re.sub(r'[^\\x00-\\x7F]+|\\x0c',' ', myfile) # remove all non-XML-compatible characters\n",
        "    p = document.add_paragraph(myfile)\n",
        "    document.save('/content/out/'+i.split('.')[0]+'.docx')"
      ],
      "metadata": {
        "id": "6QwdgFWqLndb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Emotions"
      ],
      "metadata": {
        "id": "GtIQMf3lQ4VN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "import json"
      ],
      "metadata": {
        "id": "gZcGNhV2oPDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoV5Hhw6zmSS",
        "outputId": "09fe3ce6-21b7-4512-f3ca-1140bf2aa544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import re"
      ],
      "metadata": {
        "id": "PF9OFyO-1tlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotions = [ \"fear\", \"surprise\", \"joy\", \"trust\",\n",
        "           \"sadness\", \"anticipation\", \"anger\", \"disgust\"]\n",
        "\n",
        "prompt = \"\"\" Hi, I have a text which is the auto-transcription of an interview in Finnish.\n",
        "The interview happens between two adults.\n",
        "They are talking about an experiment they did last week,\n",
        "which was teaching kids on zoom or on a VR platform. I need to identify lines containing One EMOTION\n",
        "from this text. Specifically, I need the character positions\n",
        "(beginning and end) of sentences that express The EMOTION.\n",
        "\n",
        "Please examine the text and label lines with EMOTION using this format:\n",
        "[start_index_1, end_index_1, \"EMOTION\"], [start_index_2, end_index_2\", \"EMOTION\"].\n",
        "Do not produce any extra outputs.\n",
        "Here's the text:\n",
        "\n",
        "\"\"\"\n",
        "prompt_exp = \"\"\"Hi! I have a text transcript from an experiment,\n",
        "originally in Finnish but translated into English.\n",
        "The transcript involves two kids playing gussing games\n",
        "using virtual platforms like Zoom or VR.\n",
        "I need to identify lines containing One EMOTION from this text.\n",
        "Specifically, I need the character positions\n",
        "(beginning and end) of sentences that express The EMOTION.\n",
        "\n",
        "Please examine the text and label lines with EMOTION using this format:\n",
        "[start_index_1, end_index_1, \"EMOTION\"], [start_index_2, end_index_2\", \"EMOTION\"].\n",
        "Do not produce any extra outputs.\n",
        "Here's the text:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "code_example = \"\"\"\n",
        "# code_example\n",
        "s = sentence\n",
        "# Finding the start index of the sentence and the end index\n",
        "st_idx = content.find(s)\n",
        "e_idx = st_idx + len(s)\n",
        "The EMOTION is:\n",
        "\"\"\"\n",
        "\n",
        "def process_directory_and_generate_labels(directory, output_file):\n",
        "    chunk_size = 200\n",
        "    for emotion in emotions:\n",
        "      all_labeled_files = []\n",
        "      for filename in os.listdir(directory):\n",
        "          print(filename)\n",
        "          if filename.endswith(\".srt\"):\n",
        "              file_path = os.path.join(directory, filename)\n",
        "              labeled_file = process_file_in_chunks(file_path, chunk_size, emotion)\n",
        "              all_labeled_files.append(labeled_file)\n",
        "          break\n",
        "      break\n",
        "      emotion_output_file = emotion + \"_\" + output_file\n",
        "      with open(emotion_output_file, 'w', encoding='utf-8') as f:\n",
        "          for item in all_labeled_files:\n",
        "              f.write(json.dumps(item) + '\\n')\n",
        "\n",
        "      print(f\"Generated JSONL file: {emotion_output_file}\")\n",
        "\n",
        "def extract_labels(processed_chunk, start_index, emotion):\n",
        "    labels = []\n",
        "\n",
        "    # Clean up the processed_chunk to extract only the label part\n",
        "    processed_chunk = processed_chunk.strip()\n",
        "    # Use regex to find all label occurrences in the format [start_index, end_index, \"EMOTION\"]\n",
        "    my_regex = '\\[(\\d+), (\\d+), \"' + emotion + '\"\\]'\n",
        "    pattern = re.compile(r'\\[(\\d+), (\\d+), \"%s\"\\]' % emotion)\n",
        "    #pattern = re.compile(r'^.*%s_\\d{4}-\\d{2}-\\d{2}.csv$' % re.escape(file_name_var))\n",
        "    matches = pattern.findall(processed_chunk)\n",
        "\n",
        "    for match in matches:\n",
        "        st_idx = int(match[0]) + start_index\n",
        "        e_idx = int(match[1]) + start_index\n",
        "        labels.append([st_idx, e_idx, emotion])\n",
        "\n",
        "    return labels\n",
        "\n",
        "def process_file_in_chunks(file_path, chunk_size, emotion):\n",
        "    \"\"\"Process the file in chunks with a specified size.\"\"\"\n",
        "    output = []\n",
        "    file_content = \"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            file_content = file.readlines()\n",
        "\n",
        "        # Process in chunks\n",
        "        for i in range(0, len(file_content), chunk_size):\n",
        "            chunk = \"\".join(file_content[i:i+chunk_size])\n",
        "            processed_chunk = process_chunk(chunk, emotion)  # Define process_chunk to use the API\n",
        "            start_index = \"\".join(file_content).find(chunk)\n",
        "            labels = extract_labels(processed_chunk, start_index, emotion)\n",
        "            output.extend(labels)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    return {'text': \"\".join(file_content), 'label': output}\n",
        "\n",
        "def process_chunk(chunk_text, emotion):\n",
        "\n",
        "    content = sent_tokenize(chunk_text)\n",
        "    processed_text = \"\"\n",
        "    stream = client.chat.completions.create(\n",
        "          model= \"gpt-3.5-turbo-1106\", #\"gpt-4\",\n",
        "          messages=[{\"role\": \"user\", \"content\":f\"{prompt}{content}{code_example}{emotion}\"}],\n",
        "          stream=True,\n",
        "    )\n",
        "    for chunk in stream:\n",
        "          if chunk.choices[0].delta.content is not None:\n",
        "              processed_text += chunk.choices[0].delta.content\n",
        "    print(processed_text)\n",
        "    return processed_text\n",
        "\n",
        "process_directory_and_generate_labels('/content/Interviews', 'output.jsonl')"
      ],
      "metadata": {
        "id": "aoM1qhjvQ7Xb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "118095d1-12a8-4665-9fa6-7c5520318c4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT_translated_corrected_2_interview.srt\n",
            "The text does not contain explicit mentions of emotions or feelings.\n",
            "There are no lines containing a specific emotion in this text.\n",
            "[107, 239, \"annoyed\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def merge_labels(directory, output_file):\n",
        "    text_label_map = {}\n",
        "\n",
        "    # Iterate through each file in the directory\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".jsonl\"):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                for line in f:\n",
        "                    data = json.loads(line.strip())\n",
        "                    text = data['text']\n",
        "                    labels = data['label']\n",
        "\n",
        "                    if text not in text_label_map:\n",
        "                        text_label_map[text] = []\n",
        "\n",
        "                    text_label_map[text].extend(labels)\n",
        "\n",
        "    # Write merged results to the output file\n",
        "    with open(output_file, 'w', encoding='utf-8') as f_out:\n",
        "        for text, labels in text_label_map.items():\n",
        "            merged_data = {\"text\": text, \"label\": labels}\n",
        "            f_out.write(json.dumps(merged_data) + '\\n')\n",
        "\n",
        "    print(f\"Merged JSONL file created: {output_file}\")\n",
        "\n",
        "# Example usage\n",
        "directory = '/content/'  # Replace with your directory path\n",
        "output_file = 'merged_output.jsonl'  # Replace with your output file path\n",
        "merge_labels(directory, output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rCRHxQ_EnWm",
        "outputId": "9146745a-428b-4da7-c33d-b70f95318283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged JSONL file created: merged_output.jsonl\n"
          ]
        }
      ]
    }
  ]
}