{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Agreement Metrics"
      ],
      "metadata": {
        "id": "mCBkKrd__CUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jsonlines\n",
        "import jsonlines\n",
        "from nltk.metrics import agreement"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPiPMTgcgiVz",
        "outputId": "be70125b-35b6-40f7-e3dc-6264d3505b44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jsonlines\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.2.0)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_annotations(file_path):\n",
        "    annotations = []\n",
        "    with jsonlines.open(file_path) as reader:\n",
        "        for obj in reader:\n",
        "            annotations.append(obj)\n",
        "    return annotations\n",
        "\n",
        "def prepare_data(annotations, annotator_id):\n",
        "    prepared_data = []\n",
        "    for ann in annotations:\n",
        "        # Ensure 'item' and 'label' are converted to strings to be hashable\n",
        "        item = str(ann['text'])\n",
        "        label = str(ann['label'])\n",
        "        prepared_data.append((annotator_id, item, label))\n",
        "    return prepared_data"
      ],
      "metadata": {
        "id": "YHuPyIgWgm1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlH_fQXhgcxO",
        "outputId": "b8738fbd-6372-476c-c422-61009b93df56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "control.jsonl  vs  EN_ann.jsonl \n",
            "\n",
            "Kappa: 1.0\n",
            "Fleiss' Kappa: 1.0\n",
            "Pi: 1.0\n",
            "Alpha: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Load annotations from different annotators\n",
        "file1 = 'control.jsonl'\n",
        "file2 = 'EN_ann.jsonl'\n",
        "\n",
        "annotations_annotator1 = load_annotations(file1)\n",
        "annotations_annotator2 = load_annotations(file2)\n",
        "# Repeat for other annotators as needed\n",
        "\n",
        "# Prepare data\n",
        "data_annotator1 = prepare_data(annotations_annotator1, 'annotator1')\n",
        "data_annotator2 = prepare_data(annotations_annotator1, 'annotator2')\n",
        "# Repeat for other annotators as needed\n",
        "\n",
        "# Combine all the data\n",
        "all_data = data_annotator1 + data_annotator2  # + other annotators' data\n",
        "\n",
        "# Create the AnnotationTask\n",
        "task = agreement.AnnotationTask(all_data)\n",
        "\n",
        "# Print various agreement measures\n",
        "print(file1, ' vs ', file2, '\\n')\n",
        "print(\"Kappa:\", task.kappa())\n",
        "print(\"Fleiss' Kappa:\", task.multi_kappa())\n",
        "print(\"Pi:\", task.pi())\n",
        "print(\"Alpha:\", task.alpha())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx\n",
        "!pip install nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3W_CobS8wHvq",
        "outputId": "cf10c860-1626-4dba-b183-28440a29ecc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ero speaker counter"
      ],
      "metadata": {
        "id": "fKRcZiRZ_Iyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import docx\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Ensure you have the required nltk data\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb-XBzkZwape",
        "outputId": "d5e41fe2-1f17-45db-ae5b-8e9d9c39923f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_marked_sentences_in_file(docx_file):\n",
        "    # Load the DOCX file\n",
        "    doc = docx.Document(docx_file)\n",
        "\n",
        "    # Extract text from paragraphs\n",
        "    full_text = []\n",
        "    for para in doc.paragraphs:\n",
        "        full_text.append(para.text)\n",
        "\n",
        "    # Join all paragraphs into a single string and tokenize into sentences\n",
        "    text = ' '.join(full_text)\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Initialize counters and marker\n",
        "    section_marker = None\n",
        "    section_count = 0\n",
        "    half_count = 0\n",
        "\n",
        "    # Process each sentence\n",
        "    for sentence in sentences:\n",
        "        if '§' in sentence:\n",
        "            section_marker = '§'\n",
        "            section_count += 1\n",
        "        elif '½' in sentence:\n",
        "            section_marker = '½'\n",
        "            half_count += 1\n",
        "        elif section_marker == '§':\n",
        "            section_count += 1\n",
        "        elif section_marker == '½':\n",
        "            half_count += 1\n",
        "\n",
        "    return section_count, half_count\n",
        "\n",
        "def count_marked_sentences_in_directory(directory):\n",
        "    total_section_count = 0\n",
        "    total_half_count = 0\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".docx\"):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            section_count, half_count = count_marked_sentences_in_file(file_path)\n",
        "            total_section_count += section_count\n",
        "            total_half_count += half_count\n",
        "\n",
        "    print(f\"Total number of sentences marked with §: {total_section_count}\")\n",
        "    print(f\"Total number of sentences marked with ½: {total_half_count}\")\n",
        "\n",
        "# Usage example\n",
        "count_marked_sentences_in_directory('/content/')\n"
      ],
      "metadata": {
        "id": "3deB6A829Aju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba22675a-cbdd-4c09-8b09-9f1beade1576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of sentences marked with §: 3\n",
            "Total number of sentences marked with ½: 98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eero Speaker to jsonl"
      ],
      "metadata": {
        "id": "_GStpFKZ4x5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import docx\n",
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Ensure you have the required nltk data\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuqnmK2V5gB-",
        "outputId": "c3da6ba4-209c-4656-ad79-d5ff7e5f3d82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_srt_timestamp(line):\n",
        "    # Regular expression to match SRT timestamps\n",
        "    is_srt = bool(re.match(r'^\\d+$', line)) or bool(re.match(r'^\\d{2}:\\d{2}:\\d{2},\\d{3}\\s+-->\\s+\\d{2}:\\d{2}:\\d{2},\\d{3}$', line))\n",
        "    return is_srt\n",
        "\n",
        "def process_sentences_in_file(docx_file):\n",
        "    doc = docx.Document(docx_file)\n",
        "    full_text = []\n",
        "    skip = True\n",
        "    for para in doc.paragraphs:\n",
        "        text = para.text.strip()\n",
        "        if text.endswith('.txt'):\n",
        "            skip = False\n",
        "            continue\n",
        "        if skip:\n",
        "            continue\n",
        "        lines = text.split('\\n')\n",
        "        cleaned_lines = [line for line in lines if not is_srt_timestamp(line)]\n",
        "        cleaned_text = ' '.join(cleaned_lines).strip()\n",
        "        if cleaned_text:\n",
        "            full_text.append(cleaned_text)\n",
        "\n",
        "    text = ' '.join(full_text)\n",
        "    sentences = sent_tokenize(text)\n",
        "    section_marker = None\n",
        "    labels = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        start_idx = text.find(sentence)\n",
        "        end_idx = start_idx + len(sentence)\n",
        "        if '§' in sentence:\n",
        "            section_marker = 'interviewer'\n",
        "        elif '½' in sentence:\n",
        "            section_marker = 'interviewee'\n",
        "        if section_marker:\n",
        "            labels.append([start_idx, end_idx, section_marker])\n",
        "\n",
        "    return {'text': text, 'cats': [], 'Comments': [], 'label': labels}\n",
        "\n",
        "\n",
        "def process_directory_and_generate_jsonl(directory, output_file):\n",
        "    all_labeled_sentences = []\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".docx\"):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            labeled_sentences = process_sentences_in_file(file_path)\n",
        "            all_labeled_sentences.append(labeled_sentences)\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for item in all_labeled_sentences:\n",
        "            f.write(json.dumps(item) + '\\n')\n",
        "\n",
        "    print(f\"Generated JSONL file: {output_file}\")\n",
        "\n",
        "# Usage example\n",
        "process_directory_and_generate_jsonl('/content/', '/content/speaker-Eero.jsonl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guO18KPgRhub",
        "outputId": "6de22caa-d895-4f20-f8a7-b38f45aa0024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated JSONL file: /content/speaker-Eero.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###### seem to be NOT latest version\n",
        "\n",
        "def is_srt_timestamp(line):\n",
        "    # Regular expression to match SRT timestamps (e.g., \"2\" or \"00:00:14,100 --> 00:00:15,000\")\n",
        "    is_srt = bool(re.match(r'^\\d+$', line)) or bool(re.match(r'^\\d{2}:\\d{2}:\\d{2},\\d{3}\\s+-->\\s+\\d{2}:\\d{2}:\\d{2},\\d{3}$', line))\n",
        "\n",
        "    return is_srt\n",
        "\n",
        "def process_sentences_in_file(docx_file):\n",
        "    # Load the DOCX file\n",
        "    doc = docx.Document(docx_file)\n",
        "\n",
        "    # Extract text from paragraphs and skip unwanted parts\n",
        "    full_text = []\n",
        "    skip = True\n",
        "    for para in doc.paragraphs:\n",
        "        text = para.text.strip()\n",
        "        # Skip lines that are .txt filenames\n",
        "        if text.endswith('.txt'):\n",
        "            skip = False\n",
        "            continue\n",
        "        if skip:\n",
        "          continue\n",
        "        # Skip lines that are SRT timestamps but keep the rest of the text\n",
        "        lines = text.split('\\n')\n",
        "        #print(lines)\n",
        "        cleaned_lines = [line for line in lines if not is_srt_timestamp(line)]\n",
        "        cleaned_text = ' '.join(cleaned_lines).strip()\n",
        "        if cleaned_text:\n",
        "            full_text.append(cleaned_text)\n",
        "\n",
        "    # Join all paragraphs into a single string and tokenize into sentences\n",
        "    text = ' '.join(full_text)\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Initialize marker and list to store labeled sentences\n",
        "    section_marker = None\n",
        "    labeled_sentences = []\n",
        "\n",
        "    # Process each sentence\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if '§' in sentence:\n",
        "            section_marker = 'interviewer'\n",
        "        elif '½' in sentence:\n",
        "            section_marker = 'interviewee'\n",
        "\n",
        "        if section_marker:\n",
        "            labeled_sentences.append({\n",
        "                'text': sentence,\n",
        "                'label': section_marker\n",
        "            })\n",
        "\n",
        "    return labeled_sentences\n",
        "\n",
        "def process_directory_and_generate_jsonl(directory, output_file):\n",
        "    all_labeled_sentences = []\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".docx\"):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            labeled_sentences = process_sentences_in_file(file_path)\n",
        "            all_labeled_sentences.extend(labeled_sentences)\n",
        "\n",
        "    # Write the results to a JSONL file\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for sentence in all_labeled_sentences:\n",
        "            f.write(json.dumps(sentence) + '\\n')\n",
        "\n",
        "    print(f\"Generated JSONL file: {output_file}\")\n",
        "\n",
        "# Usage example\n",
        "process_directory_and_generate_jsonl('/content/', '/content/speaker-Eero.jsonl')\n"
      ],
      "metadata": {
        "id": "q489h92T4w-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eero Emotion Extraction"
      ],
      "metadata": {
        "id": "71R86OcBspQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx\n",
        "\n",
        "import re\n",
        "import os\n",
        "import csv\n",
        "from docx import Document\n",
        "from docx.oxml.ns import qn"
      ],
      "metadata": {
        "id": "S5p5hC2Rsxfr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "308a34b2-2feb-499b-da98-d2ff5552ee5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_highlight_color(run):\n",
        "    highlight = run.font.highlight_color\n",
        "    if highlight:\n",
        "        return highlight\n",
        "    return None\n",
        "\n",
        "def is_end_of_sentence(text):\n",
        "    return re.search(r'[.!?]', text) is not None\n",
        "\n",
        "def extract_highlighted_text(doc_path):\n",
        "    doc = Document(doc_path)\n",
        "    highlighted_data = []\n",
        "    current_text = \"\"\n",
        "    current_color = None\n",
        "    found_txt = False\n",
        "\n",
        "\n",
        "    for para in doc.paragraphs:\n",
        "\n",
        "        if not found_txt:\n",
        "            if '.txt' in para.text:\n",
        "                found_txt = True\n",
        "                # Skip up to and including the occurrence of \".txt\"\n",
        "                para.text = para.text.split('.txt', 1)[1]\n",
        "            else:\n",
        "                continue\n",
        "        for run in para.runs:\n",
        "            color = get_highlight_color(run)\n",
        "            if color:\n",
        "                if color == current_color:\n",
        "                    current_text += run.text\n",
        "                else:\n",
        "                    if current_text:\n",
        "                        highlighted_data.append({\n",
        "                            \"text\": current_text.strip(),\n",
        "                            \"color\": current_color\n",
        "                        })\n",
        "                    current_text = run.text\n",
        "                    current_color = color\n",
        "            else:\n",
        "                if current_text:\n",
        "                    highlighted_data.append({\n",
        "                        \"text\": current_text.strip(),\n",
        "                        \"color\": current_color\n",
        "                    })\n",
        "                    current_text = \"\"\n",
        "                    current_color = None\n",
        "\n",
        "    if current_text:\n",
        "        highlighted_data.append({\n",
        "            \"text\": current_text.strip(),\n",
        "            \"color\": current_color\n",
        "        })\n",
        "\n",
        "    sentence_highlighted_data = []\n",
        "\n",
        "    for item in highlighted_data:\n",
        "        text = item[\"text\"]\n",
        "        sentences = re.split(r'(?<=[,.!?]) +', text)\n",
        "        for sentence in sentences:\n",
        "            sentence_highlighted_data.append({\n",
        "                \"text\": sentence.strip(),\n",
        "                \"color\": item[\"color\"]\n",
        "            })\n",
        "\n",
        "    return sentence_highlighted_data\n",
        "\n",
        "# List of document paths\n",
        "doc_paths = ['/content/1_interview.docx', '/content/2_interview.docx', '/content/3_interview.docx', '/content/4_interview.docx']\n",
        "\n",
        "doc_paths2 = ['/content/1.docx', '/content/A1_Hololens.docx',\n",
        "             '/content/B1_camera.docx', '/content/C2_HoloLens.docx',\n",
        "             '/content/2.docx', '/content/A2_Hololens.docx',\n",
        "             '/content/B2_camera.docx', '/content/D1_camera.docx',\n",
        "             '/content/3.docx', '/content/A3_Hololens.docx',\n",
        "             '/content/C1_Hololens.docx', '/content/D2_phone.docx']  # Add your document paths here\n",
        "\n",
        "# Extract highlighted text and color from multiple documents\n",
        "all_highlighted_data = []\n",
        "\n",
        "for doc_path in doc_paths:\n",
        "    highlighted_data = extract_highlighted_text(doc_path)\n",
        "    for data in highlighted_data:\n",
        "        all_highlighted_data.append({\n",
        "            \"document\": os.path.basename(doc_path),\n",
        "            \"highlighted_text\": data[\"text\"],\n",
        "            \"color\": data[\"color\"]\n",
        "        })\n",
        "\n",
        "# Write the highlighted text and color to a CSV file\n",
        "csv_file = 'highlighted_text.csv'\n",
        "with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.DictWriter(file, fieldnames=[\"Document\", \"Highlighted Text\", \"Color\"])\n",
        "    writer.writeheader()\n",
        "    for entry in all_highlighted_data:\n",
        "        writer.writerow({\n",
        "            \"Document\": entry[\"document\"],\n",
        "            \"Highlighted Text\": entry[\"highlighted_text\"],\n",
        "            \"Color\": entry[\"color\"]\n",
        "        })\n",
        "\n",
        "print(f\"Highlighted text and colors have been extracted and saved to {csv_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-cYYRMtst1T",
        "outputId": "8986a1be-d23f-43f3-ce06-2ce9f56bef5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Highlighted text and colors have been extracted and saved to highlighted_text.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Counting"
      ],
      "metadata": {
        "id": "cO7jhqIRyFvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from collections import defaultdict\n",
        "\n",
        "# Read the highlighted text and colors from the CSV file\n",
        "input_csv_file = 'highlighted_text.csv'\n",
        "color_counts = defaultdict(int)\n",
        "\n",
        "with open(input_csv_file, mode='r', encoding='utf-8') as file:\n",
        "    reader = csv.DictReader(file)\n",
        "\n",
        "    # Skip the first 41 rows\n",
        "    for _ in range(41):\n",
        "        next(reader, None)\n",
        "\n",
        "    for row in reader:\n",
        "        color = row['Color']\n",
        "        color_counts[color] += 1\n",
        "\n",
        "# Write the color counts to a new CSV file\n",
        "output_csv_file = 'color_counts.csv'\n",
        "with open(output_csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Color\", \"Count\"])\n",
        "    for color, count in color_counts.items():\n",
        "        writer.writerow([color, count])\n",
        "\n",
        "print(f\"Color counts have been extracted and saved to {output_csv_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnqqsENDyH9A",
        "outputId": "c20d535e-3f5e-4e14-ac59-bfec73b26909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Color counts have been extracted and saved to color_counts.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JsonL"
      ],
      "metadata": {
        "id": "jP1VSHB-vKkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from docx import Document\n",
        "from docx.oxml.ns import qn\n",
        "\n",
        "def is_srt_timestamp(line):\n",
        "    # Regular expression to match SRT timestamps\n",
        "    is_srt = bool(re.match(r'^\\d+$', line)) or bool(re.match(r'^\\d{2}:\\d{2}:\\d{2},\\d{3}\\s+-->\\s+\\d{2}:\\d{2}:\\d{2},\\d{3}$', line))\n",
        "    return is_srt\n",
        "\n",
        "# Function to check if a run is highlighted and get its color\n",
        "def get_highlight_color(run):\n",
        "    highlight = run.font.element.xpath('.//w:highlight')\n",
        "    if highlight:\n",
        "        return highlight[0].get(qn('w:val'))\n",
        "    return None\n",
        "\n",
        "# Function to extract highlighted text and color from a document\n",
        "def extract_highlighted_text_with_labels(doc_path, label_colors):\n",
        "    doc = Document(doc_path)\n",
        "    highlighted_data = []\n",
        "    text = \"\"\n",
        "    annotations = []\n",
        "    current_position = 0\n",
        "    skip = True\n",
        "\n",
        "    for para in doc.paragraphs:\n",
        "\n",
        "        text = para.text.strip()\n",
        "        if text.endswith('.txt'):\n",
        "            skip = False\n",
        "            continue\n",
        "        if skip or is_srt_timestamp(text):\n",
        "            continue\n",
        "\n",
        "\n",
        "        for run in para.runs:\n",
        "            color = get_highlight_color(run)\n",
        "            if color:\n",
        "                label = label_colors.get(color, \"unknown\")\n",
        "                if label == \"unknown\":\n",
        "                  print(color)\n",
        "                annotations.append([current_position, current_position + len(run.text), label])\n",
        "            text += run.text\n",
        "            current_position += len(run.text)\n",
        "\n",
        "    return text, annotations\n",
        "\n",
        "# Define the label colors based on the user's input\n",
        "label_colors = {\n",
        "    'red': 'Anger',\n",
        "    'gray': 'Trust',\n",
        "    'lightGray': 'Trust',\n",
        "    'darkYellow': 'Anticipation',\n",
        "    'yellow': 'Joy',\n",
        "    'green': 'Disgust',\n",
        "    'darkGreen': 'Disgust',\n",
        "    'teal': 'Surprise',\n",
        "    'darkCyan': 'Surprise',\n",
        "    'pink': 'Fear',\n",
        "    'magenta': 'Fear',\n",
        "    'cyan': 'Sadness'\n",
        "}\n",
        "\n",
        "# List of document paths\n",
        "doc_folder = '/content/'  # Change this to your documents folder path\n",
        "doc_paths = [os.path.join(doc_folder, file) for file in os.listdir(doc_folder) if file.endswith('.docx')]\n",
        "\n",
        "# Extract highlighted text and annotations from multiple documents\n",
        "all_annotations_data = []\n",
        "\n",
        "for doc_path in doc_paths:\n",
        "    text, annotations = extract_highlighted_text_with_labels(doc_path, label_colors)\n",
        "    all_annotations_data.append({\n",
        "        \"document\": os.path.basename(doc_path),\n",
        "        \"text\": text,\n",
        "        \"label\": annotations\n",
        "    })\n",
        "\n",
        "# Write the annotations with labels to a JSON Lines (jsonl) file\n",
        "jsonl_file_with_labels = 'highlighted_annotations_with_labels.jsonl'\n",
        "with open(jsonl_file_with_labels, mode='w', encoding='utf-8') as file:\n",
        "    for entry in all_annotations_data:\n",
        "        json.dump(entry, file)\n",
        "        file.write('\\n')\n",
        "\n",
        "print(f\"Highlighted annotations with labels have been extracted and saved to {jsonl_file_with_labels}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etwdd88OwYEW",
        "outputId": "2177cda9-6410-4f89-f3ac-695973d66375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Highlighted annotations with labels have been extracted and saved to highlighted_annotations_with_labels.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT Speaker Counter"
      ],
      "metadata": {
        "id": "3Qug-PUC_Yg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "\n",
        "def count_sentences_in_file(file_path, interviewer_label=\"interviewer\"):\n",
        "    interviewer_sentences = 0\n",
        "    other_sentences = 0\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    for line in lines:\n",
        "        # Check if the line is spoken by the interviewer\n",
        "        if line.strip().startswith(f\"{interviewer_label}:\"):\n",
        "            # Count the number of sentences by splitting at periods\n",
        "            interviewer_sentences += len(re.findall(r'\\b[^,.?!]*[,.?!]', line.strip()))\n",
        "        elif re.match(r'^[A-Za-z]+:', line.strip()):\n",
        "            # Count the number of sentences by splitting at periods for others\n",
        "            other_sentences += len(re.findall(r'\\b[^,.?!]*[,.?!]', line.strip()))\n",
        "\n",
        "    return interviewer_sentences, other_sentences\n",
        "\n",
        "def count_sentences_in_files(file_paths, interviewer_label=\"interviewer\"):\n",
        "    total_interviewer_sentences = 0\n",
        "    total_other_sentences = 0\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        interviewer_sentences, other_sentences = count_sentences_in_file(file_path, interviewer_label)\n",
        "        total_interviewer_sentences += interviewer_sentences\n",
        "        total_other_sentences += other_sentences\n",
        "        print(f\"File: {file_path} -> Interviewer: {interviewer_sentences}, Others: {other_sentences}\")\n",
        "\n",
        "    return total_interviewer_sentences, total_other_sentences\n",
        "\n",
        "# Example usage\n",
        "directory = '/content/'\n",
        "interviewer_label = 'Speaker1'  # Change this if your interviewer label is different\n",
        "\n",
        "# Get list of all .srt files in the directory\n",
        "file_paths = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.srt')]\n",
        "\n",
        "total_interviewer_sentences, total_other_sentences = count_sentences_in_files(file_paths, interviewer_label)\n",
        "print(f\"Total number of sentences spoken by the interviewer: {total_interviewer_sentences}\")\n",
        "print(f\"Total number of sentences spoken by others: {total_other_sentences}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfkNJdCT_f4E",
        "outputId": "fdf3c6e9-dd6a-4e56-b548-2aa762ee23f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: /content/Maked_corrected_3.srt -> Interviewer: 35, Others: 29\n",
            "File: /content/Maked_corrected_1.srt -> Interviewer: 48, Others: 38\n",
            "File: /content/Maked_corrected_2.srt -> Interviewer: 18, Others: 2\n",
            "Total number of sentences spoken by the interviewer: 101\n",
            "Total number of sentences spoken by others: 69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tomi Annotation fixing"
      ],
      "metadata": {
        "id": "0GY05X-QLRcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "def count_sentences(text_segment):\n",
        "    sentences = re.split(r'[.!?]+', text_segment)\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]  # Remove empty sentences and strip spaces\n",
        "    return len(sentences)\n",
        "\n",
        "def count_sentences_by_label(jsonl_file):\n",
        "    label_sentence_counts = defaultdict(int)\n",
        "\n",
        "    # Read the JSONL file and process each line\n",
        "    with open(jsonl_file, 'r') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()  # Remove any leading/trailing whitespace\n",
        "            if not line:\n",
        "                continue  # Skip empty lines\n",
        "\n",
        "            try:\n",
        "                annotation = json.loads(line)\n",
        "                text = annotation.get(\"text\", \"\")\n",
        "                labels = annotation.get(\"label\", [])\n",
        "\n",
        "                for label_data in labels:\n",
        "                    start_idx, end_idx, label = label_data\n",
        "                    text_segment = text[start_idx:end_idx]\n",
        "                    #print(f\"Label: {label}, Text Segment: '{text_segment}'\")\n",
        "                    label_sentence_counts[label] += count_sentences(text_segment)\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "\n",
        "                print(f\"Skipping invalid JSON line: {line}\")\n",
        "            except TypeError as e:\n",
        "                print(f\"Skipping invalid data type in line: {line}\")\n",
        "\n",
        "    return label_sentence_counts\n",
        "\n",
        "# Example usage\n",
        "jsonl_file = 'admin.jsonl'\n",
        "sentence_counts = count_sentences_by_label(jsonl_file)\n",
        "print(\"Sentence counts by label:\")\n",
        "print(sentence_counts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpspnaS1NSMb",
        "outputId": "76524e7c-78c0-4e79-97c5-77097981fe9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence counts by label:\n",
            "defaultdict(<class 'int'>, {'Speaker 1': 790, 'Surprise': 125, 'Disgust': 93, 'Speaker 2': 291, 'Fear': 85, 'Anticipation': 151, 'Joy': 86, 'Trust': 424, 'Sadness': 34, 'Anger': 76, 'Interviewer': 132})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT speaker to jsonl"
      ],
      "metadata": {
        "id": "IZwdfJtehoqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import json\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def process_annotated_file(srt_file):\n",
        "    with open(srt_file, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    # Normalizing the content for uniform label handling\n",
        "    content = re.sub(r\"Speaker1\", \"Speaker 1\", content)\n",
        "    content = re.sub(r\"Speaker2\", \"Speaker 2\", content)\n",
        "\n",
        "    # Use sent_tokenize to get initial sentence split, then further split by speaker labels\n",
        "    initial_sentences = sent_tokenize(content)\n",
        "    sentences = []\n",
        "    for sentence in initial_sentences:\n",
        "        # Further split each sentence by the specific speaker labels, retaining the labels\n",
        "        parts = re.split(r'(\\bSpeaker 1:\\s*|\\bSpeaker 2:\\s*|\\bInstructor:\\s*)', sentence)\n",
        "        if len(parts) > 1:\n",
        "            buffer = \"\"\n",
        "            for part in parts:\n",
        "                if part.startswith(('Speaker 1:', 'Speaker 2:', 'Instructor:')):\n",
        "                    if buffer:\n",
        "                        sentences.append(buffer.strip())\n",
        "                        buffer = part\n",
        "                    else:\n",
        "                        buffer = part\n",
        "                else:\n",
        "                    buffer += part\n",
        "            if buffer:\n",
        "                sentences.append(buffer.strip())\n",
        "        else:\n",
        "            sentences.append(sentence.strip())\n",
        "\n",
        "    modified_sentences = []\n",
        "    labels = []\n",
        "    last_label = None\n",
        "\n",
        "    for sentence in sentences:\n",
        "\n",
        "        start_idx = content.find(sentence)\n",
        "        end_idx = start_idx + len(sentence)\n",
        "\n",
        "        # Determine if the sentence starts with a speaker label and process accordingly\n",
        "        if re.match(r'^Speaker 1:', sentence):\n",
        "            label = 'S1'\n",
        "            clean_sentence = sentence[len('Speaker 1:'):].strip()\n",
        "        elif re.match(r'^Speaker 2:', sentence):\n",
        "            label = 'S2'\n",
        "            clean_sentence = sentence[len('Speaker 2:'):].strip()\n",
        "        elif re.match(r'^Instructor:', sentence):\n",
        "            label = 'Instructor'\n",
        "            clean_sentence = sentence[len('Instructor:'):].strip()\n",
        "        else:\n",
        "            clean_sentence = sentence\n",
        "            label = last_label\n",
        "\n",
        "        if label:\n",
        "            labels.append([start_idx, end_idx, label])\n",
        "            last_label = label\n",
        "\n",
        "        modified_sentences.append(clean_sentence)\n",
        "\n",
        "    text = ' '.join(modified_sentences)\n",
        "    return {'text': text, 'label': labels}\n",
        "\n",
        "def process_directory_and_generate_jsonl(directory, output_file):\n",
        "    all_labeled_sentences = []\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".srt\"):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            labeled_sentences = process_annotated_file(file_path)\n",
        "            all_labeled_sentences.append(labeled_sentences)\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for item in all_labeled_sentences:\n",
        "            f.write(json.dumps(item) + '\\n')\n",
        "\n",
        "    print(f\"Generated JSONL file: {output_file}\")\n",
        "\n",
        "# Usage example\n",
        "process_directory_and_generate_jsonl('/content/', '/content/output.jsonl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54C-p-zDhtRj",
        "outputId": "40ee15b2-fc1c-47de-ce43-e4ca85576b3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instructor: So to speak, yes.\n",
            "\n",
            "Instructor: So, who will go first is up to you and then just go through it all. We'll pick you up in 10 minutes.\n",
            "\n",
            "Speaker 1: Can you see me? \n",
            "Speaker 2: Yeah, I see you, good.\n",
            "\n",
            "Speaker 1: Who starts? Am I supposed to perform?\n",
            "\n",
            "Speaker 1: So...\n",
            "Speaker 2: Well, I have this drawing here.\n",
            "\n",
            "Speaker 2: My game is on mute, and I don't feel like replying. But I have pictures too. Should I go first? The same place doesn't matter.\n",
            "\n",
            "Speaker 1: Can you see red color on your end?\n",
            "Speaker 2: I see it.\n",
            "\n",
            "Speaker 1: Ok.\n",
            "Speaker 2: Like at home. \n",
            "Speaker 1: Yeah, yeah.\n",
            "\n",
            "Speaker 2: Uhm... Do I have to perform or should I tell? I just...\n",
            "Speaker 1: Over there.\n",
            "\n",
            "Speaker 1: Alright, all of them are embarrassing. There's a penguin here and then... No well I said ok. I'll take a bit, I can start yeah, just wait.\n",
            "\n",
            "Speaker 2: Draw something. \n",
            "Speaker 1: Yeah.\n",
            "\n",
            "Speaker 2: I guessed it.\n",
            "\n",
            "Speaker 1: Do we have the same slips?\n",
            "Speaker 2: I don't know.\n",
            "\n",
            "Speaker 1: More.\n",
            "\n",
            "Speaker 1: Are you purposely choosing black color, what do you mean purposely? \n",
            "Speaker 2: Black color, yes.\n",
            "\n",
            "Speaker 2: A monkey, good. I can't draw a monkey. \n",
            "Speaker 1: Yeah, yeah, that monkey. Yeah, good. Ok, so now when I draw, yeah.\n",
            "\n",
            "Speaker 1: Wait a bit, I'm picturing something. We don't have... You definitely won't come.\n",
            "Speaker 2: Ok.\n",
            "Speaker 1: Oh gee.\n",
            "Speaker 2: Appears.Speaker 1: Well, I can't do anything there.\n",
            "\n",
            "Speaker 2: AR.\n",
            "\n",
            "Speaker 1: What am I supposed to represent?\n",
            "\n",
            "Instructor: If earlier for example was a monkey, then it is such a task, and then the other person tries to draw, so it is a bit more complex.\n",
            "\n",
            "Speaker 1: Well, so it is.\n",
            "\n",
            "Speaker 2: Do you see me? Yeah, I see.\n",
            "\n",
            "Speaker 1: And you're watching me, yeah.\n",
            "\n",
            "Speaker 2: Okay.\n",
            "\n",
            "Speaker 1: Was that some animal? No, this is okay now.\n",
            "\n",
            "Speaker 2: How to draw this? I know what it is. I just can't draw it.\n",
            "\n",
            "Speaker 1: You should change the color. You're drawing with that eraser now. Yeah, I know, maybe blue, or black or it feels like you guessed what I represented. But yeah, I know what it is, but I'm thinking what to draw.\n",
            "\n",
            "Speaker 2: Well, this is black, as it is black.\n",
            "\n",
            "Speaker 1: Then...\n",
            "\n",
            "Speaker 2: Okay, well I don't know already. This is just like what I represented.\n",
            "\n",
            "Speaker 1: Penguin. Well, correct, okay okay, correct. Yeah yeah. What's this weapon? And then I will guess yours. Okay, so I need to represent something. From here someone, yeah hold on a bit.\n",
            "\n",
            "Speaker 2: What?\n",
            "\n",
            "Speaker 1: Oh my God.\n",
            "\n",
            "Instructor: What are we supposed to represent? Difficult?\n",
            "\n",
            "Speaker 2: Okay.\n",
            "\n",
            "Speaker 1: I haven't watched three programs. Represent a bit.\n",
            "\n",
            "Speaker 2: Here.\n",
            "\n",
            "Speaker 1: I’m drawing. I'm drawing right now.\n",
            "\n",
            "Speaker 2: It appears in bottom corner. I can't see the drawing properly. Oh dear, then. Well, I guessed it was like...\n",
            "\n",
            "Speaker 1: Like that. A raven. No, I can move this.\n",
            "\n",
            "Speaker 2: You can zoom with two fingers. Yes, indeed you can. I have such an option here. Well, can you guess?\n",
            "\n",
            "Speaker 1: I tried to draw a bird. What's that? Well it was... well it was a duck. A duck.Speaker 1: I thought it was a crow.\n",
            "\n",
            "Speaker 2: Okay, well you don't have to keep up since I can go on for a long time.\n",
            "\n",
            "Speaker 1: Like this.\n",
            "\n",
            "Speaker 2: Okay.\n",
            "\n",
            "Speaker 1: Yes.\n",
            "\n",
            "Speaker 2: What the heck is that?\n",
            "\n",
            "Speaker 1: You probably won't guess.\n",
            "\n",
            "Speaker 2: Is it some animal since I have other papers here? It's one of your animals... ok, but it isn't a penguin, no.\n",
            "\n",
            "Speaker 1: What if I...\n",
            "\n",
            "Instructor: What are you drawing? That didn't quite go as ...\n",
            "\n",
            "Speaker 2: That looks nice.\n",
            "\n",
            "Speaker 1: This one turned out a bit too big.\n",
            "\n",
            "Speaker 2: What is it? Is it a giraffe?\n",
            "\n",
            "Speaker 1: No.\n",
            "\n",
            "Speaker 2: Look.\n",
            "\n",
            "Speaker 1: Is it a zebra?\n",
            "\n",
            "Speaker 2: Dinosaur? No.\n",
            "\n",
            "Instructor: Well, how in the world... I can't draw it. Can you tell me what it would have been? A kangaroo, that would have been my guess. Almost mine. I was trying to mimic its bounce.\n",
            "330 381\n",
            "Generated JSONL file: /content/output.jsonl\n"
          ]
        }
      ]
    }
  ]
}